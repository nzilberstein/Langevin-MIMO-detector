{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ace7c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "#\n",
    "####                               IMPORTING\n",
    "#\n",
    "################################################################################\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "from sample_generator import sample_generator\n",
    "from iterative_classifier import iterative_classifier\n",
    "\n",
    "\n",
    "################################################################################\n",
    "#\n",
    "####                              SETTINGS\n",
    "#\n",
    "################################################################################\n",
    "\n",
    "######################\n",
    "###  General setup ###\n",
    "######################\n",
    "torch.manual_seed(123)\n",
    "np.random.seed(123)\n",
    "\n",
    "useGPU = True # If true, and GPU is available, use it.\n",
    "#\\\\\\ Determine processing unit:\n",
    "if useGPU and torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    device = 'cuda'\n",
    "else:\n",
    "    device = 'cpu'\n",
    "\n",
    "######################\n",
    "### Parameter of the system and model ###\n",
    "######################\n",
    "NR = 64\n",
    "NT_list = np.arange(16, 33)\n",
    "# NT_list = np.arange(4,8)\n",
    "NT_prob = NT_list/NT_list.sum()\n",
    "mod_n = 16\n",
    "d_transmitter_encoding = NR\n",
    "d_model = 512\n",
    "n_head = 8\n",
    "nhid = d_model*4\n",
    "nlayers = 16\n",
    "dropout = 0.0\n",
    "\n",
    "epoch_size = 5000\n",
    "train_iter = 130*epoch_size\n",
    "\n",
    "# Batch sizes for training and validation sets\n",
    "train_batch_size = 256\n",
    "mini_validtn_batch_size = 5000\n",
    "\n",
    "learning_rate = 1e-4\n",
    "\n",
    "corr_flag = True\n",
    "batch_corr = True\n",
    "rho_low = 0.55\n",
    "rho_high = 0.75\n",
    "\n",
    "validtn_NT_list = np.asarray([16, 32])\n",
    "snrdb_list = {16:np.arange(11.0, 22.0), 32:np.arange(16.0, 27.0)}\n",
    "factor_list = (validtn_NT_list/validtn_NT_list.sum())/snrdb_list[16].size\n",
    "\n",
    "model_filename = './validtn_results/model.pth'\n",
    "# curr_accr = './validtn_results/curr_accr.txt'\n",
    "load_pretrained_model = False\n",
    "save_interim_model = True\n",
    "save_to_file = False\n",
    "\n",
    "################################################################################\n",
    "#\n",
    "####                              MAIN\n",
    "#\n",
    "################################################################################\n",
    "\n",
    "\n",
    "######################\n",
    "### Functions ###\n",
    "######################\n",
    "def get_snr_range(NT):\n",
    "    peak = NT*(5.0/16.0) + 6.0\n",
    "    snr_low = peak\n",
    "    snr_high = peak+10.0\n",
    "    return (snr_low, snr_high)\n",
    "\n",
    "def accuracy(out, j_indices):\n",
    "    out = out.permute(1,2,0)\n",
    "    out = out.argmax(dim=1)\n",
    "    accuracy = (out == j_indices).sum().to(dtype=torch.float32)\n",
    "    del out\n",
    "    return accuracy/j_indices.numel()\n",
    "\n",
    "def loss_function(criterion, out, j_indices):\n",
    "    out = torch.cat(out, dim=1).permute(1,2,0)\n",
    "    j_indices = j_indices.repeat(nlayers, 1)\n",
    "    loss = criterion(out, j_indices)\n",
    "    del out, j_indices\n",
    "    return loss\n",
    "\n",
    "def validate_model_given_data(model, validtn_H, validtn_y, validtn_j_indices, validtn_noise_sigma, device, criterion=None):\n",
    "    with torch.no_grad():\n",
    "\n",
    "        validtn_H = validtn_H.to(device=device).float()\n",
    "        validtn_y = validtn_y.to(device=device).float()\n",
    "        validtn_noise_sigma = validtn_noise_sigma.to(device=device).float()\n",
    "        validtn_out = model.forward(validtn_H, validtn_y, validtn_noise_sigma)\n",
    "\n",
    "        if (criterion):\n",
    "            validtn_j_indices = validtn_j_indices.to(device=device)\n",
    "            loss = loss_function(criterion, validtn_out, validtn_j_indices)\n",
    "            validtn_j_indices = validtn_j_indices.to(device='cpu')\n",
    "\n",
    "        validtn_out = validtn_out[-1].to(device='cpu')\n",
    "        accr = accuracy(validtn_out, validtn_j_indices)\n",
    "\n",
    "        del validtn_H, validtn_y, validtn_noise_sigma, validtn_out, validtn_j_indices\n",
    "\n",
    "        if (criterion):\n",
    "            return accr, loss.item()\n",
    "        else:\n",
    "            return accr, None\n",
    "\n",
    "def mini_validation(model, mini_validation_dict, i, device, criterion=None, save_to_file=True):\n",
    "    result_dict = {int(NT):{} for NT in validtn_NT_list}\n",
    "    loss_list = []\n",
    "    for index,NT in enumerate(validtn_NT_list):\n",
    "        for snr in snrdb_list[NT]:\n",
    "            big_validtn_H, big_validtn_y, big_validtn_j_indices, big_noise_sigma = mini_validation_dict[NT][snr]\n",
    "            accr, loss = validate_model_given_data(model, big_validtn_H, big_validtn_y, big_validtn_j_indices, big_noise_sigma, device, criterion)\n",
    "            result_dict[NT][snr] = accr\n",
    "            loss_list.append(loss*factor_list[index])\n",
    "\n",
    "    print('Validtn result, Accr for 16 : ', result_dict[16])\n",
    "    print('Validation resut, Accr for 32 : ', result_dict[32])\n",
    "    if (save_to_file):\n",
    "        with open(curr_accr, 'w') as f:\n",
    "            print((i, result_dict), file=f)\n",
    "        print('Saved intermediate validation results at : ', curr_accr)\n",
    "\n",
    "    if (criterion):\n",
    "        return np.sum(loss_list)\n",
    "\n",
    "def generate_big_validtn_data(generator, batch_size, corr_flag, rho, batch_corr, rho_low, rho_high):\n",
    "    validtn_data_dict = {int(NT):{} for NT in validtn_NT_list}\n",
    "    for NT in validtn_NT_list:\n",
    "        for snr in snrdb_list[NT]:\n",
    "            big_validtn_H, big_validtn_y, big_validtn_j_indices, big_noise_sigma = generator.give_batch_data(int(NT), snr_db_min=snr, snr_db_max=snr, batch_size=batch_size, correlated_flag=corr_flag, rho=rho, batch_corr=batch_corr,rho_low=rho_low, rho_high=rho_high)\n",
    "            validtn_data_dict[int(NT)][snr] = (big_validtn_H, big_validtn_y , big_validtn_j_indices, big_noise_sigma)\n",
    "    return validtn_data_dict\n",
    "\n",
    "def save_model_func(model, optimizer):\n",
    "    torch.save({'model_state_dict': model.state_dict(), 'optimizer_state_dict': optimizer.state_dict()}, model_filename)\n",
    "    print('******Model Saved********** at directory : ', model_filename)\n",
    "\n",
    "\n",
    "def train(model, optimizer, lr_scheduler, generator , device='cpu'):\n",
    "\n",
    "    mini_validation_dict = generate_big_validtn_data(generator, mini_validtn_batch_size, corr_flag, None, batch_corr, rho_low, rho_high)\n",
    "    # Fix loss criterion\n",
    "    criterion = nn.CrossEntropyLoss().to(device=device)\n",
    "    model.train()\n",
    "    epoch_count = 1\n",
    "\n",
    "    for i in range(1, train_iter+1):\n",
    "\n",
    "        # Randomly select number of transmitters\n",
    "        NT = np.random.choice(NT_list, p=NT_prob)\n",
    "        rho = np.random.triangular(rho_low, rho_high, rho_high)\n",
    "\n",
    "        snr_low, snr_high = get_snr_range(NT)\n",
    "        H, y, j_indices, noise_sigma = generator.give_batch_data(NT, snr_db_min=snr_low, snr_db_max=snr_high, batch_size=None, correlated_flag=corr_flag, rho=rho)\n",
    "\n",
    "        H = H.to(device=device).float()\n",
    "        y = y.to(device=device).float()\n",
    "        noise_sigma = noise_sigma.to(device=device).float()\n",
    "\n",
    "        out = model.forward(H,y, noise_sigma)\n",
    "\n",
    "        del H, y, noise_sigma\n",
    "\n",
    "        j_indices = j_indices.to(device=device)\n",
    "        loss = loss_function(criterion, out, j_indices)\n",
    "        del j_indices, out\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss_item = loss.item()\n",
    "        del loss\n",
    "\n",
    "        if (i%epoch_size==0):\n",
    "            print('iteration number : ', i, 'Epoch : ', epoch_count, 'User : ', NT, 'loss : ', loss_item)\n",
    "            print('Now validating')\n",
    "\n",
    "            model.eval()\n",
    "            mini_validtn_loss = mini_validation(model, mini_validation_dict, i, device, criterion, save_to_file)\n",
    "            print('Mini validation loss : ', mini_validtn_loss)\n",
    "            lr_scheduler.step(mini_validtn_loss)\n",
    "\n",
    "            model.train()\n",
    "            if (save_interim_model):\n",
    "                save_model_func(model, optimizer)\n",
    "\n",
    "            epoch_count = epoch_count+1\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "613389be",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "#\n",
    "####                              MAIN RUN\n",
    "#\n",
    "################################################################################\n",
    "\n",
    "generator = sample_generator(train_batch_size, mod_n, NR)\n",
    "model = iterative_classifier(d_model, n_head, nhid, nlayers, mod_n, NR, d_transmitter_encoding, generator.real_QAM_const, generator.imag_QAM_const, generator.constellation, device, dropout)\n",
    "model = model.to(device=device)\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=learning_rate)\n",
    "\n",
    "if (load_pretrained_model):\n",
    "    checkpoint = torch.load(model_filename)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,'min', 0.91, 0, 0.0001, 'rel', 0, 0, 1e-08, verbose = True)\n",
    "    print('*******Successfully loaded pre-trained model***********')\n",
    "else:\n",
    "    lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,'min', 0.91, 0, 0.0001, 'rel', 0, 0, 1e-08, verbose = True)\n",
    "\n",
    "train(model, optimizer, lr_scheduler, generator, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da8d1b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "#\n",
    "####                              MAIN RUN - TESTING\n",
    "#\n",
    "################################################################################\n",
    "\n",
    "\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "\n",
    "# Parameters\n",
    "NR = 64\n",
    "NT = 32\n",
    "mod_n = 16\n",
    "d_transmitter_encoding = NR\n",
    "d_model = 512\n",
    "n_head = 8\n",
    "nhid = d_model*4\n",
    "nlayers = 16\n",
    "dropout = 0.0\n",
    "\n",
    "# Batch sizes for training and validation sets\n",
    "validtn_batch_size = 5000\n",
    "validtn_iter = 500\n",
    "\n",
    "M = int(np.sqrt(mod_n))\n",
    "sigConst = np.linspace(-M+1, M-1, M) \n",
    "sigConst /= np.sqrt((sigConst ** 2).mean())\n",
    "sigConst /= np.sqrt(2.) #Each complex transmitted signal will have two parts\n",
    "\n",
    "validtn_NT_list = np.asarray([32, 32])\n",
    "snrdb_list = {16:np.arange(11.0, 19.0), 32:np.arange(16.0, 23.0)}\n",
    "corr_list = np.asarray([0.60, 0.70])\n",
    "\n",
    "corr_flag = True\n",
    "save_result = False\n",
    "\n",
    "validtn_filename = './final_results/network_fullcorr_validtn_results.pickle'\n",
    "model_filename = './validtn_results/model.pth'\n",
    "\n",
    "data_filename = str(Path(os.getcwd()).parent.absolute().parent.absolute()) + '/data/H_5000bs_3264'\n",
    "\n",
    "def accuracy(out, j_indices):\n",
    "    out = out.permute(1,2,0)\n",
    "    out = out.argmax(dim=1)\n",
    "    accuracy = (out == j_indices).sum().to(dtype=torch.float32)\n",
    "    return accuracy.item()/out.numel()\n",
    "\n",
    "def bit_indices(indices, mod_n):\n",
    "    real_indices = (indices//np.sqrt(mod_n)).to(dtype=torch.int32)\n",
    "    imag_indices = (indices%np.sqrt(mod_n)).to(dtype=torch.int32)\n",
    "    joint_bit_indices = torch.cat((real_indices, imag_indices), dim=-1)\n",
    "    return joint_bit_indices\n",
    "\n",
    "def sym_accuracy(out, j_indices):\n",
    "    accuracy = (out == j_indices).sum().to(dtype=torch.float32)\n",
    "    return accuracy.item()/out.numel()\n",
    "\n",
    "def bit_accuracy(out, j_indices):\n",
    "    out = out.permute(1,2,0)\n",
    "    out = out.argmax(dim=1)\n",
    "    bit_out_indices = bit_indices(out, mod_n)\n",
    "    bit_j_indices = bit_indices(j_indices, mod_n)\n",
    "    return sym_accuracy(bit_out_indices, bit_j_indices)\n",
    "\n",
    "def validate_model_given_data(model, validtn_H, validtn_y, validtn_j_indices, validtn_noise_sigma, device):\n",
    "    with torch.no_grad():\n",
    "\n",
    "        validtn_H = validtn_H.to(device=device).float()\n",
    "        validtn_y = validtn_y.to(device=device).float()\n",
    "        validtn_noise_sigma = validtn_noise_sigma.to(device=device).float()\n",
    "        validtn_out = model.forward(validtn_H, validtn_y, validtn_noise_sigma)\n",
    "\n",
    "        validtn_out = validtn_out[-1].to(device='cpu')\n",
    "        accr = accuracy(validtn_out, validtn_j_indices)\n",
    "\n",
    "        del validtn_H, validtn_y, validtn_out, validtn_noise_sigma\n",
    "\n",
    "    return accr\n",
    "\n",
    "\n",
    "def validate_model(model, generator, device, save_result=True):\n",
    "    result_dict = {int(NT):{rho:defaultdict(float) for rho in corr_list} for NT in validtn_NT_list}\n",
    "    for iter in range(validtn_iter):\n",
    "        validtn_data_dict = generate_big_validtn_data(generator, validtn_batch_size)\n",
    "        for NT in validtn_NT_list:\n",
    "            for rho in corr_list:\n",
    "                for snr in snrdb_list[NT]:\n",
    "                    big_validtn_H, big_validtn_y, big_validtn_j_indices, big_noise_sigma = validtn_data_dict[NT][rho][snr]\n",
    "                    accr = validate_model_given_data(model, big_validtn_H, big_validtn_y, big_validtn_j_indices, big_noise_sigma, device)\n",
    "                    result_dict[NT][rho][snr] =  result_dict[NT][rho][snr] + (accr-result_dict[NT][rho][snr])/float(iter+1.0)\n",
    "\n",
    "        if (save_result):\n",
    "            with open(validtn_filename, 'wb') as handle:\n",
    "                pickle.dump(result_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "            print('Intermediate Test results saved at : ', validtn_filename)\n",
    "        print('Big Validtn result, Accr for 16 : ', result_dict[32])\n",
    "        print('Big Validation resut, Accr for 32 : ', result_dict[32])\n",
    "\n",
    "\n",
    "def generate_big_validtn_data(generator, batch_size):\n",
    "    validtn_data_dict = {int(NT):{rho:{} for rho in corr_list} for NT in validtn_NT_list}\n",
    "    with open(data_filename, 'rb') as fp:\n",
    "        big_validtn_H = pkl.load(fp)\n",
    "    for NT in validtn_NT_list:\n",
    "        for rho in corr_list:\n",
    "            for snr in snrdb_list[NT]:\n",
    "                # big_validtn_H, big_validtn_y, big_validtn_j_indices, big_noise_sigma = generator.give_batch_data(int(NT), snr_db_min=snr, snr_db_max=snr, batch_size=batch_size, correlated_flag=corr_flag, rho=rho)\n",
    "                big_validtn_y, x, big_validtn_j_indices, big_noise_sigma = generator.give_batch_data_Hinput(big_validtn_H, int(NT), snr_db_min=snr, snr_db_max=snr, batch_size=batch_size)\t\n",
    "                validtn_data_dict[int(NT)][rho][snr] = (big_validtn_H, big_validtn_y , big_validtn_j_indices, big_noise_sigma)\n",
    "    return validtn_data_dict\n",
    "\n",
    "def test(model, generator, device):\n",
    "    model.eval()\n",
    "\n",
    "    # Testing Trained Network\n",
    "    validate_model(model, generator, device, save_result)\n",
    "\n",
    "generator = sample_generator(validtn_batch_size, mod_n, NR)\n",
    "device = 'cuda'\n",
    "model = iterative_classifier(d_model, n_head, nhid, nlayers, mod_n, NR, d_transmitter_encoding, generator.real_QAM_const, generator.imag_QAM_const, generator.constellation, device, dropout)\n",
    "model = model.to(device=device)\n",
    "\n",
    "checkpoint = torch.load(model_filename)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "print('*******Successfully loaded pre-trained model*********** from directory : ', model_filename)\n",
    "\n",
    "test(model, generator, device)\n",
    "print('******************************** Now Testing **********************************************')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be21b6ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ac4117b4f16cf43a7b4d29a08f995b98bcec8327806e376c489c9fe767f71064"
  },
  "kernelspec": {
   "display_name": "Python (torchenv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
