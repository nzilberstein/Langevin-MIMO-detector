{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "specialized-pregnancy",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "from sample_generator import sample_generator\n",
    "from iterative_classifier import iterative_classifier\n",
    "\n",
    "# Parameters\n",
    "NR = 64\n",
    "NT_list = np.arange(16, 33)\n",
    "# NT_list = np.arange(4,8)\n",
    "NT_prob = NT_list/NT_list.sum()\n",
    "mod_n = 16\n",
    "d_transmitter_encoding = NR\n",
    "d_model = 512\n",
    "n_head = 8\n",
    "nhid = d_model*4\n",
    "nlayers = 16\n",
    "dropout = 0.0\n",
    "\n",
    "epoch_size = 5000\n",
    "train_iter = 130*epoch_size\n",
    "# train_iter = 50001\n",
    "\n",
    "\n",
    "# Batch sizes for training and validation sets\n",
    "train_batch_size = 256\n",
    "mini_validtn_batch_size = 5000\n",
    "\n",
    "learning_rate = 1e-4\n",
    "\n",
    "corr_flag = True\n",
    "batch_corr = True\n",
    "rho_low = 0.55\n",
    "rho_high = 0.75\n",
    "\n",
    "validtn_NT_list = np.asarray([16, 32])\n",
    "snrdb_list = {16:np.arange(11.0, 22.0), 32:np.arange(16.0, 27.0)}\n",
    "factor_list = (validtn_NT_list/validtn_NT_list.sum())/snrdb_list[16].size\n",
    "\n",
    "model_filename = './validtn_results/model.pth'\n",
    "# curr_accr = './validtn_results/curr_accr.txt'\n",
    "load_pretrained_model = False\n",
    "save_interim_model = True\n",
    "save_to_file = False\n",
    "\n",
    "def get_snr_range(NT):\n",
    "    peak = NT*(5.0/16.0) + 6.0\n",
    "    snr_low = peak\n",
    "    snr_high = peak+10.0\n",
    "    return (snr_low, snr_high)\n",
    "\n",
    "def accuracy(out, j_indices):\n",
    "    out = out.permute(1,2,0)\n",
    "    out = out.argmax(dim=1)\n",
    "    accuracy = (out == j_indices).sum().to(dtype=torch.float32)\n",
    "    del out\n",
    "    return accuracy/j_indices.numel()\n",
    "\n",
    "def loss_function(criterion, out, j_indices):\n",
    "    out = torch.cat(out, dim=1).permute(1,2,0)\n",
    "    j_indices = j_indices.repeat(nlayers, 1)\n",
    "    loss = criterion(out, j_indices)\n",
    "    del out, j_indices\n",
    "    return loss\n",
    "\n",
    "def validate_model_given_data(model, validtn_H, validtn_y, validtn_j_indices, validtn_noise_sigma, device, criterion=None):\n",
    "    with torch.no_grad():\n",
    "\n",
    "        validtn_H = validtn_H.to(device=device).float()\n",
    "        validtn_y = validtn_y.to(device=device).float()\n",
    "        validtn_noise_sigma = validtn_noise_sigma.to(device=device).float()\n",
    "        validtn_out = model.forward(validtn_H, validtn_y, validtn_noise_sigma)\n",
    "\n",
    "        if (criterion):\n",
    "            validtn_j_indices = validtn_j_indices.to(device=device)\n",
    "            loss = loss_function(criterion, validtn_out, validtn_j_indices)\n",
    "            validtn_j_indices = validtn_j_indices.to(device='cpu')\n",
    "\n",
    "        validtn_out = validtn_out[-1].to(device='cpu')\n",
    "        accr = accuracy(validtn_out, validtn_j_indices)\n",
    "\n",
    "        del validtn_H, validtn_y, validtn_noise_sigma, validtn_out, validtn_j_indices\n",
    "\n",
    "        if (criterion):\n",
    "            return accr, loss.item()\n",
    "        else:\n",
    "            return accr, None\n",
    "\n",
    "def mini_validation(model, mini_validation_dict, i, device, criterion=None, save_to_file=True):\n",
    "    result_dict = {int(NT):{} for NT in validtn_NT_list}\n",
    "    loss_list = []\n",
    "    for index,NT in enumerate(validtn_NT_list):\n",
    "        for snr in snrdb_list[NT]:\n",
    "            big_validtn_H, big_validtn_y, big_validtn_j_indices, big_noise_sigma = mini_validation_dict[NT][snr]\n",
    "            accr, loss = validate_model_given_data(model, big_validtn_H, big_validtn_y, big_validtn_j_indices, big_noise_sigma, device, criterion)\n",
    "            result_dict[NT][snr] = accr\n",
    "            loss_list.append(loss*factor_list[index])\n",
    "\n",
    "    print('Validtn result, Accr for 16 : ', result_dict[16])\n",
    "    print('Validation resut, Accr for 32 : ', result_dict[32])\n",
    "    if (save_to_file):\n",
    "        with open(curr_accr, 'w') as f:\n",
    "            print((i, result_dict), file=f)\n",
    "        print('Saved intermediate validation results at : ', curr_accr)\n",
    "\n",
    "    if (criterion):\n",
    "        return np.sum(loss_list)\n",
    "\n",
    "def generate_big_validtn_data(generator, batch_size, corr_flag, rho, batch_corr, rho_low, rho_high):\n",
    "    validtn_data_dict = {int(NT):{} for NT in validtn_NT_list}\n",
    "    for NT in validtn_NT_list:\n",
    "        for snr in snrdb_list[NT]:\n",
    "            big_validtn_H, big_validtn_y, big_validtn_j_indices, big_noise_sigma = generator.give_batch_data(int(NT), snr_db_min=snr, snr_db_max=snr, batch_size=batch_size, correlated_flag=corr_flag, rho=rho, batch_corr=batch_corr,rho_low=rho_low, rho_high=rho_high)\n",
    "            validtn_data_dict[int(NT)][snr] = (big_validtn_H, big_validtn_y , big_validtn_j_indices, big_noise_sigma)\n",
    "    return validtn_data_dict\n",
    "\n",
    "def save_model_func(model, optimizer):\n",
    "    torch.save({'model_state_dict': model.state_dict(), 'optimizer_state_dict': optimizer.state_dict()}, model_filename)\n",
    "    print('******Model Saved********** at directory : ', model_filename)\n",
    "\n",
    "\n",
    "def train(model, optimizer, lr_scheduler, generator , device='cpu'):\n",
    "\n",
    "    mini_validation_dict = generate_big_validtn_data(generator, mini_validtn_batch_size, corr_flag, None, batch_corr, rho_low, rho_high)\n",
    "    # Fix loss criterion\n",
    "    criterion = nn.CrossEntropyLoss().to(device=device)\n",
    "    model.train()\n",
    "    epoch_count = 1\n",
    "\n",
    "    for i in range(1, train_iter+1):\n",
    "\n",
    "        # Randomly select number of transmitters\n",
    "        NT = np.random.choice(NT_list, p=NT_prob)\n",
    "        rho = np.random.triangular(rho_low, rho_high, rho_high)\n",
    "\n",
    "        snr_low, snr_high = get_snr_range(NT)\n",
    "        H, y, j_indices, noise_sigma = generator.give_batch_data(NT, snr_db_min=snr_low, snr_db_max=snr_high, batch_size=None, correlated_flag=corr_flag, rho=rho)\n",
    "\n",
    "        H = H.to(device=device).float()\n",
    "        y = y.to(device=device).float()\n",
    "        noise_sigma = noise_sigma.to(device=device).float()\n",
    "\n",
    "        out = model.forward(H,y, noise_sigma)\n",
    "\n",
    "        del H, y, noise_sigma\n",
    "\n",
    "        j_indices = j_indices.to(device=device)\n",
    "        loss = loss_function(criterion, out, j_indices)\n",
    "        del j_indices, out\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss_item = loss.item()\n",
    "        del loss\n",
    "\n",
    "        if (i%epoch_size==0):\n",
    "            print('iteration number : ', i, 'Epoch : ', epoch_count, 'User : ', NT, 'loss : ', loss_item)\n",
    "            print('Now validating')\n",
    "\n",
    "            model.eval()\n",
    "            mini_validtn_loss = mini_validation(model, mini_validation_dict, i, device, criterion, save_to_file)\n",
    "            print('Mini validation loss : ', mini_validtn_loss)\n",
    "            lr_scheduler.step(mini_validtn_loss)\n",
    "\n",
    "            model.train()\n",
    "            if (save_interim_model):\n",
    "                save_model_func(model, optimizer)\n",
    "\n",
    "            epoch_count = epoch_count+1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "solid-trash",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = sample_generator(train_batch_size, mod_n, NR)\n",
    "device = 'cuda'\n",
    "model = iterative_classifier(d_model, n_head, nhid, nlayers, mod_n, NR, d_transmitter_encoding, generator.real_QAM_const, generator.imag_QAM_const, generator.constellation, device, dropout)\n",
    "model = model.to(device=device)\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=learning_rate)\n",
    "\n",
    "if (load_pretrained_model):\n",
    "    checkpoint = torch.load(model_filename)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,'min', 0.91, 0, 0.0001, 'rel', 0, 0, 1e-08, verbose = True)\n",
    "    print('*******Successfully loaded pre-trained model***********')\n",
    "else:\n",
    "    lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,'min', 0.91, 0, 0.0001, 'rel', 0, 0, 1e-08, verbose = True)\n",
    "\n",
    "train(model, optimizer, lr_scheduler, generator, device)\n",
    "print('******************************** Now Testing **********************************************')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "composite-scoop",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*******Successfully loaded pre-trained model*********** from directory :  ./validtn_results/model.pth\n",
      "Big Validation resut, Accr for 32 :  {0.6: defaultdict(<class 'float'>, {15.0: 0.8600625, 16.0: 0.93795625, 17.0: 0.97898125, 18.0: 0.99459375, 19.0: 0.99876875, 20.0: 0.99955, 21.0: 0.9999125, 22.0: 1.0, 23.0: 1.0}), 0.7: defaultdict(<class 'float'>, {15.0: 0.8586625, 16.0: 0.93445, 17.0: 0.9803375, 18.0: 0.99456875, 19.0: 0.999, 20.0: 0.9998375, 21.0: 0.99993125, 22.0: 1.0, 23.0: 0.9999875})}\n",
      "Big Validation resut, Accr for 32 :  {0.6: defaultdict(<class 'float'>, {15.0: 0.8609437499999999, 16.0: 0.936878125, 17.0: 0.9804859375, 18.0: 0.9950953125, 19.0: 0.9990921875000001, 20.0: 0.9997703125, 21.0: 0.9999781249999999, 22.0: 1.0, 23.0: 0.999925}), 0.7: defaultdict(<class 'float'>, {15.0: 0.8592203125, 16.0: 0.9347546875000001, 17.0: 0.97803125, 18.0: 0.9944328124999999, 19.0: 0.9992296875, 20.0: 0.9997203125, 21.0: 0.999978125, 22.0: 0.9999953125000001, 23.0: 0.999996875})}\n",
      "Big Validation resut, Accr for 32 :  {0.6: defaultdict(<class 'float'>, {15.0: 0.8593256944444444, 16.0: 0.9375291666666666, 17.0: 0.9808652777777778, 18.0: 0.9945909722222223, 19.0: 0.9989854166666667, 20.0: 0.9998493055555556, 21.0: 0.9999902777777777, 22.0: 0.9999756944444445, 23.0: 0.9998902777777777}), 0.7: defaultdict(<class 'float'>, {15.0: 0.8591673611111111, 16.0: 0.9359534722222224, 17.0: 0.9780208333333333, 18.0: 0.9943833333333333, 19.0: 0.9991993055555556, 20.0: 0.9997020833333333, 21.0: 0.9999868055555555, 22.0: 0.9999979166666667, 23.0: 0.9999986111111111})}\n",
      "Big Validation resut, Accr for 32 :  {0.6: defaultdict(<class 'float'>, {15.0: 0.858494140625, 16.0: 0.9368546874999999, 17.0: 0.9806453125, 18.0: 0.9948027343750001, 19.0: 0.9991257812500001, 20.0: 0.9997648437500001, 21.0: 0.99999453125, 22.0: 0.9999863281250001, 23.0: 0.99993828125}), 0.7: defaultdict(<class 'float'>, {15.0: 0.859208984375, 16.0: 0.9357359375000001, 17.0: 0.978654296875, 18.0: 0.99493203125, 19.0: 0.99913671875, 20.0: 0.999750390625, 21.0: 0.9999652343750001, 22.0: 0.9999988281250001, 23.0: 0.99999921875})}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-65c9aa8ffa91>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'*******Successfully loaded pre-trained model*********** from directory : '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'******************************** Now Testing **********************************************'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-65c9aa8ffa91>\u001b[0m in \u001b[0;36mtest\u001b[0;34m(model, generator, device)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m     \u001b[0;31m# Testing Trained Network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m     \u001b[0mvalidate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0mgenerator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidtn_batch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmod_n\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-65c9aa8ffa91>\u001b[0m in \u001b[0;36mvalidate_model\u001b[0;34m(model, generator, device, save_result)\u001b[0m\n\u001b[1;32m     78\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0msnr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msnrdb_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mNT\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m                     \u001b[0mbig_validtn_H\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbig_validtn_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbig_validtn_j_indices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbig_noise_sigma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidtn_data_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mNT\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrho\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msnr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m                     \u001b[0maccr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate_model_given_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbig_validtn_H\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbig_validtn_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbig_validtn_j_indices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbig_noise_sigma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m                     \u001b[0mresult_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mNT\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrho\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msnr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mresult_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mNT\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrho\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msnr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0maccr\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mresult_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mNT\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrho\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msnr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-65c9aa8ffa91>\u001b[0m in \u001b[0;36mvalidate_model_given_data\u001b[0;34m(model, validtn_H, validtn_y, validtn_j_indices, validtn_noise_sigma, device)\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0mvalidtn_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidtn_y\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0mvalidtn_noise_sigma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidtn_noise_sigma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m         \u001b[0mvalidtn_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidtn_H\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidtn_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidtn_noise_sigma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0mvalidtn_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidtn_out\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/GNN_project/learning_based/Torch/Langevin/re-mimo/iterative_classifier.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, H, y, noise_sigma, attn_mask, save_attn_weight)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_decoder\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterative_blocks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m             \u001b[0msout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder_decoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnoise_sigma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_attn_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m             \u001b[0mx_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/GNN_project/learning_based/Torch/Langevin/re-mimo/EncoderDecoderBlock.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, st, xt, H, y, noise_sigma, NT, index, attn_weights, save_attn_weight)\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnoise_sigma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_attn_weight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0mencoder_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen_common_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnoise_sigma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0mxt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0mencoder_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_attn_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattn_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/GNN_project/learning_based/Torch/Langevin/re-mimo/EncoderDecoderBlock.py\u001b[0m in \u001b[0;36mgen_common_input\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0mdelta_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfinal_repr_encoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal_repr_decoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_weights\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnoise_sigma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_attn_weight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "import pickle as pkl\n",
    "\n",
    "# Parameters\n",
    "NR = 64\n",
    "NT = 32\n",
    "mod_n = 16\n",
    "d_transmitter_encoding = NR\n",
    "d_model = 512\n",
    "n_head = 8\n",
    "nhid = d_model*4\n",
    "nlayers = 16\n",
    "dropout = 0.0\n",
    "\n",
    "# Batch sizes for training and validation sets\n",
    "validtn_batch_size = 5000\n",
    "validtn_iter = 2000\n",
    "\n",
    "M = int(np.sqrt(mod_n))\n",
    "sigConst = np.linspace(-M+1, M-1, M) \n",
    "sigConst /= np.sqrt((sigConst ** 2).mean())\n",
    "sigConst /= np.sqrt(2.) #Each complex transmitted signal will have two parts\n",
    "\n",
    "validtn_NT_list = np.asarray([32, 32])\n",
    "snrdb_list = {16:np.arange(11.0, 19.0), 32:np.arange(15.0, 24.0)}\n",
    "corr_list = np.asarray([0.60, 0.70])\n",
    "\n",
    "corr_flag = True\n",
    "\n",
    "validtn_filename = './final_results/network_fullcorr_validtn_results.pickle'\n",
    "model_filename = './validtn_results/model.pth'\n",
    "\n",
    "def accuracy(out, j_indices):\n",
    "    out = out.permute(1,2,0)\n",
    "    out = out.argmax(dim=1)\n",
    "    accuracy = (out == j_indices).sum().to(dtype=torch.float32)\n",
    "    return accuracy.item()/out.numel()\n",
    "\n",
    "def bit_indices(indices, mod_n):\n",
    "    real_indices = (indices//np.sqrt(mod_n)).to(dtype=torch.int32)\n",
    "    imag_indices = (indices%np.sqrt(mod_n)).to(dtype=torch.int32)\n",
    "    joint_bit_indices = torch.cat((real_indices, imag_indices), dim=-1)\n",
    "    return joint_bit_indices\n",
    "\n",
    "def sym_accuracy(out, j_indices):\n",
    "    accuracy = (out == j_indices).sum().to(dtype=torch.float32)\n",
    "    return accuracy.item()/out.numel()\n",
    "\n",
    "def bit_accuracy(out, j_indices):\n",
    "    out = out.permute(1,2,0)\n",
    "    out = out.argmax(dim=1)\n",
    "    bit_out_indices = bit_indices(out, mod_n)\n",
    "    bit_j_indices = bit_indices(j_indices, mod_n)\n",
    "    return sym_accuracy(bit_out_indices, bit_j_indices)\n",
    "\n",
    "def validate_model_given_data(model, validtn_H, validtn_y, validtn_j_indices, validtn_noise_sigma, device):\n",
    "    with torch.no_grad():\n",
    "\n",
    "        validtn_H = validtn_H.to(device=device).float()\n",
    "        validtn_y = validtn_y.to(device=device).float()\n",
    "        validtn_noise_sigma = validtn_noise_sigma.to(device=device).float()\n",
    "        validtn_out = model.forward(validtn_H, validtn_y, validtn_noise_sigma)\n",
    "\n",
    "        validtn_out = validtn_out[-1].to(device='cpu')\n",
    "        accr = accuracy(validtn_out, validtn_j_indices)\n",
    "\n",
    "        del validtn_H, validtn_y, validtn_out, validtn_noise_sigma\n",
    "\n",
    "    return accr\n",
    "\n",
    "\n",
    "def validate_model(model, generator, device, save_result=True):\n",
    "    result_dict = {int(NT):{rho:defaultdict(float) for rho in corr_list} for NT in validtn_NT_list}\n",
    "    for iter in range(validtn_iter):\n",
    "        validtn_data_dict = generate_big_validtn_data(generator, validtn_batch_size)\n",
    "        for NT in validtn_NT_list:\n",
    "            for rho in corr_list:\n",
    "                for snr in snrdb_list[NT]:\n",
    "                    big_validtn_H, big_validtn_y, big_validtn_j_indices, big_noise_sigma = validtn_data_dict[NT][rho][snr]\n",
    "                    accr = validate_model_given_data(model, big_validtn_H, big_validtn_y, big_validtn_j_indices, big_noise_sigma, device)\n",
    "                    result_dict[NT][rho][snr] =  result_dict[NT][rho][snr] + (accr-result_dict[NT][rho][snr])/float(iter+1.0)\n",
    "\n",
    "        if (save_result):\n",
    "            with open(validtn_filename, 'wb') as handle:\n",
    "                pickle.dump(result_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "            print('Intermediate Test results saved at : ', validtn_filename)\n",
    "#         print('Big Validtn result, Accr for 16 : ', result_dict[16])\n",
    "        print('Big Validation resut, Accr for 32 : ', result_dict[32])\n",
    "\n",
    "\n",
    "def generate_big_validtn_data(generator, batch_size):\n",
    "    validtn_data_dict = {int(NT):{rho:{} for rho in corr_list} for NT in validtn_NT_list}\n",
    "    with open('/home/nicoz/GNN_project/learning_based/Torch/Langevin/H_5000bs_3264', 'rb') as fp:\n",
    "        big_validtn_H = pkl.load(fp)\n",
    "    for NT in validtn_NT_list:\n",
    "        for rho in corr_list:\n",
    "            for snr in snrdb_list[NT]:\n",
    "                big_validtn_y, _, big_validtn_j_indices, big_noise_sigma = generator.give_batch_data_Hinput(big_validtn_H, int(NT), snr_db_min=snr, snr_db_max=snr, batch_size = batch_size)\n",
    "#                 big_validtn_H, big_validtn_y, big_validtn_j_indices, big_noise_sigma = generator.give_batch_data(int(NT), snr_db_min=snr, snr_db_max=snr, batch_size=batch_size, correlated_flag=corr_flag, rho=rho)\n",
    "                validtn_data_dict[int(NT)][rho][snr] = (big_validtn_H, big_validtn_y , big_validtn_j_indices, big_noise_sigma)\n",
    "    return validtn_data_dict\n",
    "\n",
    "def test(model, generator, device):\n",
    "    model.eval()\n",
    "\n",
    "    # Testing Trained Network\n",
    "    validate_model(model, generator, device, False)\n",
    "\n",
    "generator = sample_generator(validtn_batch_size, mod_n, NR)\n",
    "device = 'cuda'\n",
    "model = iterative_classifier(d_model, n_head, nhid, nlayers, mod_n, NR, d_transmitter_encoding, generator.real_QAM_const, generator.imag_QAM_const, generator.constellation, device, dropout)\n",
    "model = model.to(device=device)\n",
    "\n",
    "checkpoint = torch.load(model_filename)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "print('*******Successfully loaded pre-trained model*********** from directory : ', model_filename)\n",
    "\n",
    "test(model, generator, device)\n",
    "print('******************************** Now Testing **********************************************')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "static-house",
   "metadata": {},
   "outputs": [],
   "source": [
    "def attn_mask(H, NT):\n",
    "    min_value = 1\n",
    "    attn_weights = torch.zeros((n_head * H.shape[0], NT, NT))\n",
    "    Hcomplex = H[:,0:NR,0:NT] +1j*H[:,NR:,0:NT]\n",
    "\n",
    "    corr = torch.zeros((H.shape[0], NT, NT))\n",
    "\n",
    "    for nt in range(0, NT):\n",
    "        for nt2 in range(0,NT):\n",
    "            if nt != nt2:\n",
    "                norm1 = torch.bmm(torch.conj(torch.transpose(Hcomplex[:,:,nt:nt+1],2,1)), Hcomplex[:,:,nt:nt+1])\n",
    "                norm2 = torch.bmm(torch.conj(torch.transpose(Hcomplex[:,:,nt2:nt2+1],2,1)), Hcomplex[:,:,nt2:nt2+1])\n",
    "                corr[:,nt:nt+1,nt2:nt2+1] = torch.abs(torch.divide(torch.bmm(torch.conj(torch.transpose(Hcomplex[:,:,nt2:nt2+1],2,1)), Hcomplex[:,:,nt:nt+1]), torch.sqrt(norm1*norm2)))\n",
    "\n",
    "\n",
    "    for bs in range(0, H.shape[0]):\n",
    "        for nt in range(0, NT):\n",
    "            for nt2 in range(0,NT):\n",
    "                if nt != nt2:\n",
    "        #             torch.where(corr > 0, x, 0.)\n",
    "                    if corr[bs,nt,nt2] < min_value:\n",
    "                        attn_weights[0 + n_head*bs: n_head + n_head * bs,nt,nt2] = float('-inf')\n",
    "                    else:\n",
    "                        attn_weights[0 + n_head*bs: n_head + n_head * bs,nt,nt2] = float(0.0)\n",
    "                        \n",
    "    return attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "universal-matter",
   "metadata": {},
   "outputs": [],
   "source": [
    "NT = 6\n",
    "rho = 0.6\n",
    "\n",
    "mini_validation_dict = generate_big_validtn_data(generator, mini_validtn_batch_size, corr_flag, None, batch_corr, rho_low, rho_high)\n",
    "device = 'cuda'\n",
    "criterion = nn.CrossEntropyLoss().to(device=device)\n",
    "mini_validation(model, mini_validation_dict, 0, device, criterion, save_to_file)\n",
    "del mini_validation_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "endless-ukraine",
   "metadata": {},
   "outputs": [],
   "source": [
    "validtn_batch_size = 1000\n",
    "validtn_iter = 500\n",
    "\n",
    "corr_list = np.asarray([0.60, 0.6])\n",
    "from collections import defaultdict\n",
    "\n",
    "def generate_big_validtn_data(generator, batch_size):\n",
    "    validtn_data_dict = {int(NT):{rho:{} for rho in corr_list} for NT in validtn_NT_list}\n",
    "    for NT in validtn_NT_list:\n",
    "        for rho in corr_list:\n",
    "            for snr in snrdb_list[NT]:\n",
    "                big_validtn_H, big_validtn_y, big_validtn_j_indices, big_noise_sigma = generator.give_batch_data(int(NT), snr_db_min=snr, snr_db_max=snr, batch_size=batch_size, correlated_flag=corr_flag, rho=rho)\n",
    "                validtn_data_dict[int(NT)][rho][snr] = (big_validtn_H, big_validtn_y , big_validtn_j_indices, big_noise_sigma)\n",
    "    return validtn_data_dict\n",
    "\n",
    "def validate_model_given_data(model, validtn_H, validtn_y, validtn_j_indices, validtn_noise_sigma, device):\n",
    "    with torch.no_grad():\n",
    "\n",
    "        validtn_H = validtn_H.to(device=device)\n",
    "        validtn_y = validtn_y.to(device=device)\n",
    "        validtn_noise_sigma = validtn_noise_sigma.to(device=device)\n",
    "        validtn_out = model.forward(validtn_H, validtn_y, validtn_noise_sigma)\n",
    "\n",
    "        validtn_out = validtn_out[-1].to(device='cpu')\n",
    "        accr = accuracy(validtn_out, validtn_j_indices)\n",
    "\n",
    "        del validtn_H, validtn_y, validtn_out, validtn_noise_sigma\n",
    "\n",
    "    return accr\n",
    "\n",
    "\n",
    "def validate_model(model, generator, device, save_result=True):\n",
    "    result_dict = {int(NT):{rho:defaultdict(float) for rho in corr_list} for NT in validtn_NT_list}\n",
    "    for iter in range(validtn_iter):\n",
    "        validtn_data_dict = generate_big_validtn_data(generator, validtn_batch_size)\n",
    "        for NT in validtn_NT_list:\n",
    "            for rho in corr_list:\n",
    "                for snr in snrdb_list[NT]:\n",
    "                    big_validtn_H, big_validtn_y, big_validtn_j_indices, big_noise_sigma = validtn_data_dict[NT][rho][snr]\n",
    "                    accr = validate_model_given_data(model, big_validtn_H.float(), big_validtn_y.float(), big_validtn_j_indices.float(), big_noise_sigma.float(), device)\n",
    "                    ###--------------------------------------------------------------------------------###       \n",
    "                    big_validtn_H = big_validtn_H.to(device=device).float()\n",
    "                    big_validtn_y = big_validtn_y.to(device=device).float()\n",
    "                    big_noise_sigma = big_noise_sigma.to(device=device).float()\n",
    "                    big_validtn_j_indices = big_validtn_j_indices.to(device=device)\n",
    "                    out = model.forward(big_validtn_H, big_validtn_y, big_noise_sigma)[-1].permute(1,2,0)\n",
    "                    out = out.argmax(dim=1)\n",
    "\n",
    "                    index_wrongg = []\n",
    "                    for ii in range(out.shape[0]):\n",
    "                        if torch.sum(out[ii:ii+1,:] != big_validtn_j_indices[ii:ii+1,:]) != 0:\n",
    "                            index_wrongg.append(ii)\n",
    "                    \n",
    "                    accr = (validtn_batch_size - len(index_wrongg)) / validtn_batch_size\n",
    "                    ###--------------------------------------------------------------------------------###    \n",
    "\n",
    "                    result_dict[int(NT)][rho][snr] =  result_dict[int(NT)][rho][snr] + (accr-result_dict[int(NT)][rho][snr])/float(iter+1.0)\n",
    "\n",
    "        if (save_result):\n",
    "            with open(validtn_filename, 'wb') as handle:\n",
    "                pickle.dump(result_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "            print('Intermediate Test results saved at : ', validtn_filename)\n",
    "        print('Big Validtn result, Acc for 6 : ', result_dict[6])\n",
    "#         print('Big Validation resut, Accr for 32 : ', result_dict[32])\n",
    "\n",
    "def test(model, generator, device):\n",
    "    model.eval()\n",
    "\n",
    "    # Testing Trained Network\n",
    "    validate_model(model, generator, device, False)\n",
    "\n",
    "\n",
    "\n",
    "generator = sample_generator(validtn_batch_size, mod_n, NR)\n",
    "device = 'cuda'\n",
    "# model = iterative_classifier(d_model, n_head, nhid, nlayers, mod_n, NR, d_transmitter_encoding, generator.real_QAM_const, generator.imag_QAM_const, generator.constellation, device, dropout)\n",
    "# model = model.to(device=device)\n",
    "\n",
    "# checkpoint = torch.load(model_filename)\n",
    "# model.load_state_dict(checkpoint['model_state_dict'])\n",
    "# print('*******Successfully loaded pre-trained model*********** from directory : ', model_filename)\n",
    "\n",
    "test(model, generator, device)\n",
    "print('******************************** Now Testing **********************************************')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efficient-decision",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import pickle\n",
    "import os\n",
    "import collections\n",
    "\n",
    "from sample_generator import sample_generator\n",
    "from iterative_classifier import iterative_classifier\n",
    "from matrix_models import *\n",
    "\n",
    "# Parameters\n",
    "NR = 12\n",
    "NT_list = np.arange(6,7)\n",
    "NT_prob = NT_list/NT_list.sum()\n",
    "mod_n = 4\n",
    "d_transmitter_encoding = NR\n",
    "d_model = 512\n",
    "n_head = 4\n",
    "nhid = d_model*4\n",
    "nlayers = 16\n",
    "dropout = 0.0\n",
    "\n",
    "epoch_size = 5000\n",
    "train_iter = 130*epoch_size\n",
    "\n",
    "# Batch sizes for training and validation sets\n",
    "train_batch_size = 256\n",
    "mini_validtn_batch_size = 256\n",
    "validtn_batch_size = 5000\n",
    "validtn_iter = 2000\n",
    "\n",
    "learning_rate = 1e-4\n",
    "QR = True\n",
    "corr_flag = True\n",
    "batch_corr = True\n",
    "rho_low = 0.55\n",
    "rho_high = 0.75\n",
    "\n",
    "validtn_NT_list = np.asarray([6, 6])\n",
    "snrdb_list = {6:np.arange(10.0, 21.0), 32:np.arange(16.0, 27.0)}\n",
    "factor_list = (validtn_NT_list/validtn_NT_list.sum())/snrdb_list[6].size\n",
    "\n",
    "model_filename = './validtn_results/model.pth'\n",
    "curr_accr = './validtn_results/curr_accr.txt'\n",
    "load_pretrained_model = False\n",
    "save_interim_model = False\n",
    "save_to_file = False\n",
    "\n",
    "def get_snr_range(NT):\n",
    "    peak = NT*(5.0/16.0) + 6.0\n",
    "    snr_low = peak\n",
    "    snr_high = peak+10.0\n",
    "    return (snr_low, snr_high)\n",
    "\n",
    "def accuracy(out, j_indices):\n",
    "    out = out.permute(1,2,0)\n",
    "    out = out.argmax(dim=1)\n",
    "    accuracy = (out == j_indices).sum().to(dtype=torch.float32)\n",
    "    del out\n",
    "    return accuracy/j_indices.numel()\n",
    "\n",
    "def loss_function(criterion, out, j_indices):\n",
    "    out = torch.cat(out, dim=1).permute(1,2,0)\n",
    "    j_indices = j_indices.repeat(nlayers, 1)\n",
    "    loss = criterion(out, j_indices)\n",
    "    del out, j_indices\n",
    "    return loss\n",
    "\n",
    "def validate_model_given_data(model, validtn_H, validtn_y, validtn_j_indices, validtn_noise_sigma, device, criterion=None):\n",
    "    with torch.no_grad():\n",
    "\n",
    "        validtn_H = validtn_H.to(device=device)\n",
    "        validtn_y = validtn_y.to(device=device)\n",
    "        validtn_noise_sigma = validtn_noise_sigma.to(device=device)\n",
    "        validtn_out = model.forward(validtn_H, validtn_y, validtn_noise_sigma)\n",
    "\n",
    "        if (criterion):\n",
    "            validtn_j_indices = validtn_j_indices.to(device=device)\n",
    "            loss = loss_function(criterion, validtn_out, validtn_j_indices)\n",
    "            validtn_j_indices = validtn_j_indices.to(device='cpu')\n",
    "\n",
    "        validtn_out = validtn_out[-1].to(device='cpu')\n",
    "        accr = accuracy(validtn_out, validtn_j_indices)\n",
    "\n",
    "        del validtn_H, validtn_y, validtn_noise_sigma, validtn_out, validtn_j_indices\n",
    "\n",
    "        if (criterion):\n",
    "            return accr, loss.item()\n",
    "        else:\n",
    "            return accr, None\n",
    "\n",
    "def mini_validation(model, mini_validation_dict, i, device, criterion=None, save_to_file=True):\n",
    "    result_dict = {int(NT):{} for NT in validtn_NT_list}\n",
    "    loss_list = []\n",
    "    for index,NT in enumerate(validtn_NT_list):\n",
    "        for snr in snrdb_list[NT]:\n",
    "            big_validtn_H, big_validtn_y, big_validtn_j_indices, big_noise_sigma = mini_validation_dict[NT][snr]\n",
    "            accr, loss = validate_model_given_data(model, big_validtn_H.float(), big_validtn_y.float(), big_validtn_j_indices, big_noise_sigma.float(), device, criterion)\n",
    "            result_dict[NT][snr] = accr\n",
    "            loss_list.append(loss*factor_list[index])\n",
    "\n",
    "    print('Validtn result, Accr for 6 : ', result_dict[6])\n",
    "#     print('Validation resut, Accr for 32 : ', result_dict[32])\n",
    "    if (save_to_file):\n",
    "        with open(curr_accr, 'w') as f:\n",
    "            print((i, result_dict), file=f)\n",
    "        print('Saved intermediate validation results at : ', curr_accr)\n",
    "\n",
    "    if (criterion):\n",
    "        return np.sum(loss_list)\n",
    "\n",
    "def generate_big_validtn_data(generator, batch_size, QR, Cu):\n",
    "    validtn_data_dict = {int(NT):{} for NT in validtn_NT_list}\n",
    "    for NT in validtn_NT_list:\n",
    "        for snr in snrdb_list[NT]:\n",
    "#             big_validtn_H, big_validtn_y, big_validtn_j_indices, big_noise_sigma = generator.give_batch_data(int(NT), snr_db_min=snr, snr_db_max=snr, batch_size=batch_size, correlated_flag=corr_flag, rho=rho, batch_corr=batch_corr,rho_low=rho_low, rho_high=rho_high)\n",
    "            big_validtn_H, big_validtn_y, big_validtn_x, big_validtn_j_indices, big_noise_sigma = generator.give_batch_data(int(NT), snr_db_min=snr, snr_db_max=snr, batch_size=batch_size, QR = QR, Cu = Cu)            \n",
    "            validtn_data_dict[int(NT)][snr] = (big_validtn_H, big_validtn_y , big_validtn_j_indices, big_noise_sigma)\n",
    "    return validtn_data_dict\n",
    "\n",
    "def save_model_func(model, optimizer):\n",
    "    torch.save({'model_state_dict': model.state_dict(), 'optimizer_state_dict': optimizer.state_dict()}, model_filename)\n",
    "    print('******Model Saved********** at directory : ', model_filename)\n",
    "\n",
    "\n",
    "def train(Cu, model, optimizer, lr_scheduler, generator , device='cpu'):\n",
    "    \n",
    "    mini_validation_dict = generate_big_validtn_data(generator, mini_validtn_batch_size, QR = QR, Cu = Cu)\n",
    "    # Fix loss criterion\n",
    "    criterion = nn.CrossEntropyLoss().to(device=device)\n",
    "    model.train()\n",
    "    epoch_count = 1\n",
    "    \n",
    "    Q,R,HH = createQR(Cu, train_batch_size)\n",
    "    H = torch.tensor(R).to(device=device).double()\n",
    "\n",
    "    for i in range(1, train_iter+1):\n",
    "        print(i)\n",
    "        # Randomly select number of transmitters\n",
    "        NT = np.random.choice(NT_list, p=NT_prob)\n",
    "        rho = np.random.triangular(rho_low, rho_high, rho_high)\n",
    "\n",
    "        snr_low, snr_high = get_snr_range(NT)\n",
    "#         H, y, j_indices, noise_sigma = generator.give_batch_data(NT, snr_db_min=snr_low, snr_db_max=snr_high, batch_size=None, correlated_flag=corr_flag, rho=rho)\n",
    "\n",
    "        if (i%50==0):\n",
    "            Q,R,HH = createQR(Cu, train_batch_size)\n",
    "            H = torch.tensor(R).to(device=device)\n",
    "        \n",
    "        y, x, j_indices, noise_sigma = generator.give_batch_data_Hinput(H, NT, snr_db_min=snrdb_list[NT][0], snr_db_max=snrdb_list[NT][-1], batch_size=train_batch_size)\n",
    "\n",
    "        H = H.to(device=device).float()\n",
    "        y = y.to(device=device).float()\n",
    "        noise_sigma = noise_sigma.to(device=device).float()\n",
    "\n",
    "        out = model.forward(H,y, noise_sigma)\n",
    "\n",
    "        del y, noise_sigma\n",
    "\n",
    "        j_indices = j_indices.to(device=device)\n",
    "        loss = loss_function(criterion, out, j_indices)\n",
    "        del j_indices, out\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss_item = loss.item()\n",
    "        del loss\n",
    "\n",
    "        if (i%epoch_size==0):\n",
    "            print('iteration number : ', i, 'Epoch : ', epoch_count, 'User : ', NT, 'loss : ', loss_item)\n",
    "            print('Now validating')\n",
    "\n",
    "            model.eval()\n",
    "            mini_validtn_loss = mini_validation(model, mini_validation_dict, i, device, criterion, save_to_file)\n",
    "            print('Mini validation loss : ', mini_validtn_loss)\n",
    "            lr_scheduler.step(mini_validtn_loss)\n",
    "\n",
    "            model.train()\n",
    "#             if (save_interim_model):\n",
    "#                 save_model_func(model, optimizer)\n",
    "\n",
    "            epoch_count = epoch_count+1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extensive-guyana",
   "metadata": {},
   "outputs": [],
   "source": [
    "NT = np.random.choice(NT_list, p=NT_prob)\n",
    "generator = sample_generator(train_batch_size, mod_n, NT)\n",
    "device = 'cuda'\n",
    "model = iterative_classifier(d_model, n_head, nhid, nlayers, mod_n, NT, d_transmitter_encoding, generator.real_QAM_const, generator.imag_QAM_const, generator.constellation, device, dropout)\n",
    "model = model.to(device=device)\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=learning_rate)\n",
    "\n",
    "# if (load_pretrained_model):\n",
    "#     checkpoint = torch.load(model_filename)\n",
    "#     model.load_state_dict(checkpoint['model_state_dict'])\n",
    "#     optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "#     lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,'min', 0.91, 0, True, 0.0001, 'rel', 0, 0, 1e-08)\n",
    "#     print('*******Successfully loaded pre-trained model***********')\n",
    "# else:\n",
    "lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,'min', 0.91, 0, 0.0001, 'rel', 0, 0, 1e-08, verbose = True)\n",
    "\n",
    "with open ('/home/nicoz/GNN_project/learning_based/Torch/Tests_sets/Test_set2', 'rb') as fp:\n",
    "    R_test, Cu = pkl.load(fp)\n",
    "train(Cu, model, optimizer, lr_scheduler, generator, device)\n",
    "print('******************************** Now Testing **********************************************')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "purple-petite",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(out, j_indices):\n",
    "    out = out.permute(1,2,0)\n",
    "    out = out.argmax(dim=1)\n",
    "    accuracy = (out == j_indices).sum().to(dtype=torch.float32)\n",
    "    return accuracy.item()/out.numel()\n",
    "\n",
    "def bit_indices(indices, mod_n):\n",
    "    real_indices = (indices//np.sqrt(mod_n)).to(dtype=torch.int32)\n",
    "    imag_indices = (indices%np.sqrt(mod_n)).to(dtype=torch.int32)\n",
    "    joint_bit_indices = torch.cat((real_indices, imag_indices), dim=-1)\n",
    "    return joint_bit_indices\n",
    "\n",
    "def sym_accuracy(out, j_indices):\n",
    "    accuracy = (out == j_indices).sum().to(dtype=torch.float32)\n",
    "    return accuracy.item()/out.numel()\n",
    "\n",
    "def bit_accuracy(out, j_indices):\n",
    "    out = out.permute(1,2,0)\n",
    "    out = out.argmax(dim=1)\n",
    "    bit_out_indices = bit_indices(out, mod_n)\n",
    "    bit_j_indices = bit_indices(j_indices, mod_n)\n",
    "    return sym_accuracy(bit_out_indices, bit_j_indices)\n",
    "\n",
    "def validate_model_given_data(model, validtn_H, validtn_y, validtn_j_indices, validtn_noise_sigma, device):\n",
    "    with torch.no_grad():\n",
    "\n",
    "        validtn_H = validtn_H.to(device=device).float()\n",
    "        validtn_y = validtn_y.to(device=device).float()\n",
    "        validtn_noise_sigma = validtn_noise_sigma.to(device=device).float()\n",
    "        validtn_out = model.forward(validtn_H, validtn_y, validtn_noise_sigma)\n",
    "\n",
    "        validtn_out = validtn_out[-1].to(device='cpu')\n",
    "        accr = accuracy(validtn_out, validtn_j_indices)\n",
    "\n",
    "        del validtn_H, validtn_y, validtn_out, validtn_noise_sigma\n",
    "\n",
    "    return accr\n",
    "\n",
    "\n",
    "def validate_model(model, generator, device, save_result=True, Cu = None):\n",
    "    result_dict = {int(NT): {snr: float(0) for snr in snrdb_list[6]} for NT in validtn_NT_list}    \n",
    "    for iter in range(validtn_iter):\n",
    "        validtn_data_dict = generate_big_validtn_data(generator, validtn_batch_size, QR = QR, Cu = Cu)\n",
    "#         for NT in validtn_NT_list:\n",
    "        for snr in snrdb_list[NT]:\n",
    "            big_validtn_H, big_validtn_y, big_validtn_j_indices, big_noise_sigma = validtn_data_dict[NT][snr]\n",
    "            accr = validate_model_given_data(model, big_validtn_H, big_validtn_y, big_validtn_j_indices, big_noise_sigma, device)\n",
    "            result_dict[NT][snr] = result_dict[NT][snr] + (accr-result_dict[NT][snr])/float(iter+1.0)\n",
    "#                 result_dict[NT][snr] = result_dict[NT][snr] + accr\n",
    "            print('Big Validtn result, Accr for 16 : ', accr)\n",
    "        if (save_result):\n",
    "            with open('REMIMO_result2_testSet2', 'wb') as handle:\n",
    "                pickle.dump(result_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "#             print('Intermediate Test results saved at : ', validtn_filename)\n",
    "        print('Big Validtn result, Accr for 16 : ', result_dict[6])\n",
    "    #         print('Big Validation resut, Accr for 32 : ', result_dict[32])\n",
    "\n",
    "\n",
    "def test(model, generator, device, Cu = None):\n",
    "    model.eval()\n",
    "\n",
    "    # Testing Trained Network\n",
    "    validate_model(model, generator, device, True, Cu = Cu)\n",
    "\n",
    "# corr_list = np.asarray([0.60, 0.70])\n",
    "NT = np.random.choice(NT_list, p=NT_prob)\n",
    "generator = sample_generator(validtn_batch_size, mod_n, NT)\n",
    "# device = 'cuda'\n",
    "# model = iterative_classifier(d_model, n_head, nhid, nlayers, mod_n, NT, d_transmitter_encoding, generator.real_QAM_const, generator.imag_QAM_const, generator.constellation, device, dropout)\n",
    "# model = model.to(device=device)\n",
    "\n",
    "# checkpoint = torch.load('re_mimo_localScatteringModel.pth')\n",
    "# model.load_state_dict(checkpoint)\n",
    "# print('*******Successfully loaded pre-trained model*********** from directory : ', 're_mimo_localScatteringModel.pth')\n",
    "\n",
    "test(model, generator, device, Cu = Cu)\n",
    "print('******************************** Now Testing **********************************************')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comic-senate",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nicoz-env",
   "language": "python",
   "name": "nicoz-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
